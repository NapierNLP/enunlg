defaults:
  - data/corpus: e2e-cleaned
  - _self_

random_seed: 42

preprocessing:
  mr:
    ignore_order: True
    set_position: False
  text:
    delexicalise:
      mode: split_on_caps # could also be strict
      slots: [name, near]
    normalise: tgen

pytorch:
  device: cpu

model:
  name: tgen_classifier
  text_encoder:
    embeddings:
      type: torch # could also be one-hot, word2vec, glove, zero, etc
      embedding_dim: 50
      backprop: True
      padding_idx: 0
      start_idx: 1
      stop_idx: 2
    cell: lstm
    num_hidden_dims: 128

mode: train

train:
  num_epochs: 20
  record_interval: 4120
  shuffle: True
  batch_size: 1 # TGen used 20
  optimizer: adam
  learning_rate: 0.0005
  learning_rate_decay: 0.5 # TGen used 0.0
  train_splits: [train]
  dev_splits: [dev]

test:
  test_splits: [test]
#  classifier_file: outputs/2024-03-21/14-55-01/trained_TGenSemClassifier.nlg.tgz
  classifier_file: models/e2e.semclassifier.nlg.tgz

