defaults:
  - data/corpus: e2e_train
  - _self_

random_seed: 42

preprocessing:
  mr:
    ignore_order: True
    set_position: False
  text:
    delexicalise:
      mode: split_on_caps # could also be strict
      slots: [name, near]
    normalise: tgen

pytorch:
  device: cpu

model:
  name: tgen
  max_input_length: 30
  encoder:
    embeddings:
      mode: random # could also be one-hot, word2vec, glove, zero, etc
      dimensions: 50
      backprop: True
    cell: lstm
    num_hidden_dims: 128
  decoder:
    embeddings:
      mode: random
      dimensions: 50
      backprop: True
    cell: lstm
    num_hidden_dims: 128

mode:
  train:
    num_epochs: 20
    record_interval: 1000
    shuffle: True
    batch_size: 1 # TGen used 20
    optimizer: adam
    learning_rate: 0.0005
    learning_rate_decay: 0.5 # TGen used 0.0
